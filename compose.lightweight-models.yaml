# Lightweight Models for Docker MCP Gateway Demo
# Choose models based on your available VRAM

services:
  adk:
    environment:
      # Override to use lightweight local model
      - AI_DEFAULT_MODEL=local/lightweight
      - OPENAI_MODEL_NAME=lightweight-model

# Available lightweight model configurations
# Uncomment ONE of the sections below based on your system capabilities

# ====================================================================
# ULTRA LIGHTWEIGHT - Phi3 Mini (3.8B params, ~2GB VRAM)
# ====================================================================
models:
  lightweight:
    model: ai/phi3:mini-4k-instruct-q4_0  # 2.3GB download
    # Best for: Most laptops, minimal VRAM requirements

# ====================================================================
# VERY LIGHTWEIGHT - TinyLlama (1.1B params, ~700MB VRAM)  
# ====================================================================
# models:
#   lightweight:
#     model: ai/tinyllama:1.1b-chat-v1.0-q4_0  # 700MB download
#     # Best for: Older hardware, very limited VRAM

# ====================================================================
# LIGHTWEIGHT - Llama 3.2 3B (3B params, ~2GB VRAM)
# ====================================================================
# models:
#   lightweight:
#     model: ai/llama3.2:3b-instruct-q4_0  # 2.0GB download  
#     # Best for: Good balance of size and performance

# ====================================================================
# SMALL BUT CAPABLE - Mistral 7B (7B params, ~4GB VRAM)
# ====================================================================
# models:
#   lightweight:
#     model: ai/mistral:7b-instruct-v0.3-q4_0  # 4.1GB download
#     # Best for: Systems with moderate VRAM, good performance

# ====================================================================
# CPU-ONLY OPTION - Gemma 2B (2B params, works on CPU)
# ====================================================================
# models:
#   lightweight:
#     model: ai/gemma:2b-instruct-q4_0  # 1.6GB download
#     # Best for: No GPU systems, runs on CPU

# ====================================================================
# FAST & SMALL - CodeLlama 7B Code (7B params, ~4GB VRAM)
# ====================================================================
# models:
#   lightweight:
#     model: ai/codellama:7b-code-q4_0  # 3.8GB download
#     # Best for: Code-heavy applications, good reasoning
