# Ultra-lightweight configuration for systems with very limited resources
# Uses TinyLlama 1.1B - only 700MB VRAM needed!

services:
  adk:
    environment:
      # Configure for Docker Model Runner with TinyLlama
      - AI_DEFAULT_MODEL=tinyllama
      - OPENAI_BASE_URL=http://host.docker.internal:12434/v1
      - OPENAI_MODEL_NAME=ai/tinyllama:1.1b-chat-v1.0-q4_0
    models:
      tinyllama:
        endpoint_var: OPENAI_BASE_URL
        model_var: OPENAI_MODEL_NAME

models:
  tinyllama:
    # TinyLlama 1.1B - smallest model that still works well
    model: ai/tinyllama:1.1b-chat-v1.0-q4_0
    # Download size: ~700MB
    # VRAM needed: ~700MB
    # Works on: Almost any system with basic GPU or even CPU
